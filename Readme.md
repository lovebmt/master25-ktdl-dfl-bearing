# Federated Learning for Bearing Anomaly Detection with Flower ğŸŒ¸

This notebook demonstrates federated learning using Flower (flwr) framework with PyTorch to train an autoencoder model on bearing sensor data. The autoencoder learns to reconstruct normal bearing behavior, which can be used for anomaly detection.

## ğŸ“‹ Table of Contents
1. **Setup & Installation** - Install dependencies
2. **Data Preparation** - Load and prepare bearing sensor data
   - Understanding data structure
   - Input/Output examples
   - Data statistics and visualization
3. **Model Definition** - Define the autoencoder architecture
4. **Dataset Class** - Create PyTorch dataset for autoencoder
5. **Data Loading Functions** - Partition data for federated learning
6. **Training & Testing Functions** - Define training and evaluation logic
7. **Flower Client Definition** - Define federated learning client
8. **Flower Server Strategy** - Define server aggregation strategy
9. **Run Federated Learning Simulation** - Execute FL training
10. **Evaluate Final Model** - Test the global model
11. **Visualize Results** - Plot training metrics and reconstructions
    - Loss and RMSE plots
    - Accuracy metrics and improvements
    - Reconstruction quality analysis
    - Input â†’ Output testing with examples
12. **Save Final Model** - Export trained model
13. **Summary & Next Steps** - Key insights and future directions
## ğŸ“– Project Overview

This notebook demonstrates **Federated Learning for Bearing Anomaly Detection** using an Autoencoder architecture.

### **What We'll Build:**
1. **Autoencoder Model** ğŸ§ 
   - Input: 8 sensor readings from bearing vibration data
   - Architecture: Encoder (8 â†’ 4 â†’ 2) + Decoder (2 â†’ 4 â†’ 8)
   - Purpose: Learn normal bearing patterns and detect anomalies

2. **Federated Learning with Flower** ğŸŒ¸
   - Multiple clients (simulating edge devices)
   - Collaborative training without sharing raw data
   - Privacy-preserving machine learning

3. **Two Key Experiments** ğŸ”¬
   - **Experiment 1**: FedAvg with **balanced data** (IID) - Baseline
   - **Experiment 2**: FedAvg with **imbalanced data** (Non-IID) - Real-world scenario

### **Key Research Questions:**
- â“ How does data distribution affect federated learning?
- â“ Does imbalanced data degrade FedAvg performance?

### **Technology Stack:**
- **PyTorch**: Deep learning framework
- **Flower**: Federated learning framework
- **Pandas**: Data manipulation
- **Matplotlib**: Visualization

### **Dataset:**
- **NASA IMS Bearing Dataset** (vibration sensor data)
- 8 channels of vibration measurements
- Multiple bearing failures recorded over time
- Perfect for demonstrating edge device scenarios

### **Why This Matters:**
In real-world Industrial IoT:
- ğŸ­ Different factories collect different amounts of data
- ğŸ“Š Data distribution is naturally imbalanced
- ğŸ”’ Data privacy regulations prevent centralized collection
- ğŸŒ Federated learning enables collaborative model training

Let's explore how different data distributions affect federated learning performance!
## 8. Flower Server Strategy ğŸ¯

Define aggregation strategy for Federated Learning.

### **FedAvg (Federated Averaging):**
- âœ… **Simple**: Weighted average of model parameters from clients
- âœ… **Fast**: No additional regularization term
- âœ… **Good with IID data**: Works well when data is evenly distributed
- âš ï¸ **May struggle with non-IID data**: Performance can degrade with imbalanced data

**Formula:**
```
Aggregated_Î¸ = Î£(n_i/N Ã— Î¸_i)

Where:
- Î¸_i: Model parameters from client i
- n_i: Number of samples in client i
- N: Total number of samples
```

We'll test FedAvg with both **balanced** and **imbalanced** data distributions!

### ğŸ’¡ Key Insights from Experiments

**What We Learned:**

1. **Balanced Data (Exp 1) - Baseline Performance** âœ…
   - FedAvg works well when clients have equal data
   - Stable convergence
   - Good final performance
   
2. **Imbalanced Data (Exp 2) - Real-World Challenge** âš ï¸
   - Performance may degrade with non-IID data
   - Some clients with less data may overfit
   - Convergence can be slower and less stable
   - Shows the real-world challenges of federated learning

**Recommendations for Production:**

| Scenario | Recommended Strategy | Reason |
|----------|---------------------|---------|
| Balanced data across clients | FedAvg | Simple, efficient, works well |
| Imbalanced data (real-world) | FedAvg or FedProx | Standard approaches for production |
| High data heterogeneity | FedProx or other advanced methods | Better handles non-IID |
| Limited communication | FedAvg | Lower overhead |

**Industrial IoT Applications:**
- ğŸ­ Predictive maintenance with edge devices
- ğŸŒ Distributed quality control systems
- ğŸ“± Mobile device anomaly detection
- ğŸš— Vehicle fleet health monitoring

**Key Takeaway:**
Data distribution significantly impacts federated learning performance. In real-world deployments, understanding your data distribution is critical for successful model training.

### ğŸ’¡ Giáº£i ThÃ­ch: Táº¡i Sao DÃ¹ng MSE vÃ  CÃ¡ch PhÃ¡t Hiá»‡n Báº¥t ThÆ°á»ng?

#### ğŸ¤” **CÃ¢u há»i: Táº¡i sao cáº§n tÃ­nh MSE?**

**MSE (Mean Squared Error)** Ä‘o lÆ°á»ng **sá»± khÃ¡c biá»‡t** giá»¯a:
- **Input** (giÃ¡ trá»‹ cáº£m biáº¿n gá»‘c)
- **Output** (giÃ¡ trá»‹ model reconstruct)

**CÃ´ng thá»©c MSE:**
```
MSE = (1/8) Ã— Î£(input_i - output_i)Â²

Trong Ä‘Ã³:
- 8 = sá»‘ cáº£m biáº¿n (B1_a, B1_b, B2_a, B2_b, B3_a, B3_b, B4_a, B4_b)
- input_i = giÃ¡ trá»‹ cáº£m biáº¿n thá»© i
- output_i = giÃ¡ trá»‹ reconstruct thá»© i
```

#### ğŸ¯ **MSE LiÃªn Quan Äáº¿n Báº¥t ThÆ°á»ng NhÆ° Tháº¿ NÃ o?**

**NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng:**

1. **Model há»c tá»« dá»¯ liá»‡u BÃŒNH THÆ¯á»œNG** (training):
   - Model há»c "pattern" cá»§a bearing hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng
   - VD: B1_a â‰ˆ 0.12, B1_b â‰ˆ 0.15, tÆ°Æ¡ng quan giá»¯a cÃ¡c cáº£m biáº¿n...
   
2. **Khi gáº·p dá»¯ liá»‡u BÃŒNH THÆ¯á»œNG** (testing):
   - Model **NHáº¬N RA** pattern quen thuá»™c
   - Reconstruct **CHÃNH XÃC**
   - **MSE THáº¤P** âœ…
   
3. **Khi gáº·p dá»¯ liá»‡u Báº¤T THÆ¯á»œNG** (testing):
   - Model **KHÃ”NG NHáº¬N RA** pattern nÃ y (chÆ°a há»c bao giá»)
   - Reconstruct **SAI Lá»†CH**
   - **MSE CAO** âŒ

---

#### ğŸ“Š **VÃ­ Dá»¥ Cá»¥ Thá»ƒ:**

**Case 1: Máº«u BÃ¬nh ThÆ°á»ng**
```
Input:  [0.12, 0.15, 0.11, 0.13, 0.14, 0.12, 0.10, 0.13]
Output: [0.12, 0.15, 0.11, 0.13, 0.14, 0.12, 0.10, 0.13]
        â†“
MSE = ((0.12-0.12)Â² + (0.15-0.15)Â² + ... ) / 8 = 0.0001
      â†“
âœ… MSE THáº¤P â†’ BÃŒNH THÆ¯á»œNG
```

**Case 2: Máº«u Báº¥t ThÆ°á»ng (Cáº£m biáº¿n lá»—i)**
```
Input:  [1.20, 0.15, 0.11, 0.13, 0.14, 0.12, 0.10, 0.13]  â† B1_a = 1.20 (quÃ¡ cao!)
Output: [0.18, 0.15, 0.11, 0.13, 0.14, 0.12, 0.10, 0.13]  â† Model cá»‘ reconstruct
        â†“
MSE = ((1.20-0.18)Â² + (0.15-0.15)Â² + ... ) / 8 = 0.1302
      â†“
âŒ MSE CAO â†’ Báº¤T THÆ¯á»œNG
```

---

#### ğŸ“ **TÃ³m Táº¯t:**

| TÃ¬nh Huá»‘ng | Pattern | Model Reconstruct | MSE | Káº¿t Luáº­n |
|------------|---------|-------------------|-----|----------|
| **BÃ¬nh thÆ°á»ng** | Model Ä‘Ã£ há»c | ChÃ­nh xÃ¡c âœ… | **Tháº¥p** (< threshold) | âœ… Normal |
| **Báº¥t thÆ°á»ng** | Model chÆ°a há»c | Sai lá»‡ch âŒ | **Cao** (> threshold) | ğŸš¨ Anomaly |

---

#### ğŸ”§ **á»¨ng Dá»¥ng Thá»±c Táº¿:**

**Bearing bá»‹ há»ng â†’ Rung báº¥t thÆ°á»ng â†’ MSE cao â†’ PhÃ¡t hiá»‡n ká»‹p thá»i!**

1. **BÃ¬nh thÆ°á»ng**: Bearing hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh
   - Rung Ä‘á»u â†’ MSE tháº¥p â†’ KhÃ´ng cáº£nh bÃ¡o
   
2. **Báº¯t Ä‘áº§u há»ng**: Rung báº¯t Ä‘áº§u thay Ä‘á»•i
   - MSE tÄƒng dáº§n â†’ Cáº£nh bÃ¡o sá»›m
   
3. **Há»ng náº·ng**: Rung ráº¥t báº¥t thÆ°á»ng
   - MSE ráº¥t cao â†’ Cáº£nh bÃ¡o ngay láº­p tá»©c!

ğŸ’¡ **Æ¯u Ä‘iá»ƒm**: KhÃ´ng cáº§n nhÃ£n "báº¥t thÆ°á»ng", chá»‰ cáº§n há»c tá»« dá»¯ liá»‡u bÃ¬nh thÆ°á»ng!